* TODO Normalize inputs between -1 and 1
* TODO Tanh as last layer of the generator output
* TODO Generator Loss is max log D + Preceptural Loss + cosine similarity
* TODO Use batch norm for real and fake instances
* TODO Use LeakyReLU in both G and D
* TODO Average Pooling, Conv2d + stride for downsampling
* TODO Use Label Smoothing.
* TODO (Optional) keep replay buffer of past generations and occasionally show them
* TODO Keep checkpoints from the past of G and D and occasionall swap them out for a few iterations.
* TODO Use ADAM Optimizer for Generator
* TODO Use SGD for descriminator
* TODO Make sure D loss isn't 0
* TODO if norms of gradients are over 100, things are screwed up.
* TODO add artificial noise to inputs to D
* TODO Use Dropouts in G in both train and test phase with p=0.5
* stuff to implement
** Generator architecture
*** TODO Last layer
*** TODO 
** TODO Discriminator loss
** TODO Generator loss
* Pretraining
  1. Predict the single using the average
     1. grab examples
     2. add noise
     3. loss between noisy data and true thing
  2. smooth single transition
  3. downssample, add noise, and upsample.
  4. average is the average over all the other peptides. Provides additional information for predicting the single one
* TODO to Read
** DONE Image Deraining 
   CLOSED: [2017-04-25 Tue 17:47]
   https://arxiv.org/pdf/1701.05957.pdf
   Use a symmetric structure, because need to transform into a domain which the real image and the noise can be separated, and then transferred back to original.
** DONE Perceptual Loss for real time style transfer and super-resolution
   CLOSED: [2017-04-26 Wed 15:57]
   https://arxiv.org/pdf/1603.08155.pdf
** DONE DCGAN Paper
   CLOSED: [2017-04-26 Wed 15:57]
   https://arxiv.org/pdf/1511.06434.pdf
** DONE Conditional Adversarial Generative Nets
   CLOSED: [2017-04-26 Wed 15:57]
   https://arxiv.org/pdf/1411.1784.pdf
   Use class labels / extra information in both the generator and discriminator
** A noise model for mass spectrometry based proteomics
   https://academic.oup.com/bioinformatics/article/24/8/1070/213310/A-noise-model-for-mass-spectrometry-based
** http://kvfrans.com/variational-autoencoders-explained/
* Generator
** Input
*** a set of clean activations.
** Layers
*** Symmetric?
    Used in rain so that they can get 
* Discriminator
** Input
*** a set of activations.
*** whether they're real or fake
** Loss
*** whether or not it was the right prediction.
* Things to talk about in the paper
** TODO Architecture decisions
*** TODO Used the advice given in the NIPS tutorial
*** TODO Neither the discriminator nor the generator has any pooling type layer
*** TODO Hyper parameter optimization was a problem, wanted to investigate alternet parameters but coudln't
*** TODO Used batch normalization.
*** TODO Wanted to use a perceptual loss function, but no supervised information for this data.
** TODO Discriminator Architecture
*** TODO a batch with 1/2 real (no noise) and 1/2 fake (from generator)
*** TODO SGD was used to optimize, with a learning rate of 0.01
** TODO Generator Architecture
*** TODO noisy original was given as the inputs
*** TODO ADAM with epsilon = 0.01 was used.
*** TODO No pooling layers because the necessary information is local
*** TODO The goal was to force the network to by having fewer parameters in the middle
**** TODO Similar to an autoencoder
**** TODO Idea inspired by *rain paper*
** TODO Results discussion
*** TODO Unclear whether the adversarial component made that much difference
*** TODO Really needed a perceptual loss function to get the same level of success
